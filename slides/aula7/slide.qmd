---
title: "MD21 Introdução à Estatística"
subtitle: "TCL, método Delta e Introdução à inferência"
author: "Leo Bastos"
format: 
  revealjs:
    slide-number: true
    logo: ../../impa-tech-h.png
    footer: '<https://github.com/lsbastos/md21/>'
    theme: simple
    chalkboard: true
editor: visual
---

# Teorema central do limite

## Teorema central do limite

Seja $X_1, X_2, \ldots, X_n$ variáveis aleatórias **independentes e identicamente distribuídas** com média, $\mathbb{E}[X_i] = \mu$ e variância, $\sigma^2 < \infty$. Seja $\bar{X}_n = \frac{\sum_{i=1}^{n} X_i}{n}$. Então $$Z_n = \frac{(\bar{X}_n - \mu)}{\mathbb{V}(\bar{X}_n)} = \frac{\sqrt{n}(\bar{X}_n - \mu)}{\sigma}  \xrightarrow[n\to \infty]{D} Z$$ onde $Z \sim N(0,1).$

. . .

Abuso de notação: $Z_n \xrightarrow{D} N(0,1)$

## Teorema central do limite

Uma implicação do TCL é que a distribuição de $\bar{X}_n$ converge para a normal, ou seja $$ \bar{X}_n \xrightarrow{D} X$$ onde $X \sim N(\mu, \sigma^2/n)$.

. . .

Outras implicações, que significam a mesma coisa $$Z_n \xrightarrow{D} N(0,1) \qquad  \qquad \bar{X}_n \xrightarrow{D} N(\mu,\sigma^2/n) \\
\bar{X}_n - \mu \xrightarrow{D} N(0,\sigma^2/n)  \quad  \qquad 
\sqrt{n}(\bar{X}_n - \mu) \xrightarrow{D} N(0,\sigma^2) 
$$

<!-- ## TCL (Exemplo Gama) -->

<!-- Suponha que $X_1,X_2.\ldots, X_n$ seja uma amostra aleatória com distribuição $Gama(a=4, b=2)$. Ou seja, $$\mathbb{E}[X_i] = a/b = 2, \qquad \mathbb{V}[X_i] = a/b^2 = 1.$$ -->

<!-- . . . -->

<!-- Pelo TCL, $$ \bar{X}_n \xrightarrow{D} N\left(\frac{a}{b}=2, \frac{a}{b^2n}=\frac{1}{n} \right)$$ -->

<!-- ## TCL (Visualizando) -->

<!-- ::: panel-tabset -->

<!-- ### Gama(4,2) -->

<!-- ```{r} -->

<!-- library(ggplot2) -->

<!-- # param = cbind(c(0.5, 1:4), c(0.5, rep(2,3),1)) -->

<!-- # labels <- parse(text = sprintf("alpha == %f ~ beta == %f", param[,1], param[,2])) -->

<!-- p <- ggplot() + -->

<!--   geom_function(fun = dgamma, args = list(shape = 4, rate =2), mapping = aes(colour = "Gamma(4,2)" ) ) + -->

<!--   geom_vline(xintercept = 2, linetype=2) +  -->

<!--   # geom_function(fun = dgamma, args = list(shape = param[2,1], scale = param[2,2]),  -->

<!--   #               mapping = aes(colour = "2", linetype ="2" ) ) + -->

<!--   # geom_function(fun = dgamma, args = list(shape = param[3,1], scale = param[3,2]),  -->

<!--   #               mapping = aes(colour = "3", linetype ="3" ) ) + -->

<!--   # geom_function(fun = dgamma, args = list(shape = param[4,1], scale = param[4,2]),  -->

<!--   #               mapping = aes(colour = "4", linetype ="4" ) ) + -->

<!--   # geom_function(fun = dgamma, args = list(shape = param[5,1], scale = param[5,2]),  -->

<!--   #               mapping = aes(colour = "5", linetype ="5" ) ) + -->

<!--   # scale_color_manual(values = 1:5, breaks =  1:5, labels = labels) + -->

<!--   # scale_linetype_manual(values = 1:5, breaks =  1:5, labels = labels) + -->

<!--   xlim(0,8) + -->

<!--   labs( -->

<!--     x = "x", -->

<!--     y = "f(x)", -->

<!--     colour = "", -->

<!--   ) +  -->

<!--   theme_bw(base_size = 16) +  -->

<!--     theme(legend.position = c(.8,.75)) -->

<!-- p -->

<!-- ``` -->

<!-- ### $n=10$ -->

<!-- ```{r} -->

<!-- p + -->

<!--   # xlim(0,4) + -->

<!--   geom_function(fun = dnorm, args = list(mean = 2, sd =sqrt(2/10)), mapping = aes(colour = "n=10; N(2,1/n)" ) )   -->

<!-- ``` -->

<!-- ### $n=50$ -->

<!-- ```{r} -->

<!-- p + -->

<!--   # xlim(0,4) + -->

<!--   geom_function(fun = dnorm, args = list(mean = 2, sd =sqrt(2/10)), mapping = aes(colour = "n=10; N(2,1/n)" ) ) + -->

<!--   geom_function(fun = dnorm, args = list(mean = 2, sd =sqrt(2/30)), mapping = aes(colour = "n=30; N(2,1/n)" ) )   -->

<!-- ``` -->

<!-- ### $n=100$ -->

<!-- ```{r} -->

<!-- p + -->

<!--   # xlim(0,4) + -->

<!--   geom_function(fun = dnorm, args = list(mean = 2, sd =sqrt(2/10)), mapping = aes(colour = "n=10; N(2,1/n)" ) ) + -->

<!--   geom_function(fun = dnorm, args = list(mean = 2, sd =sqrt(2/30)), mapping = aes(colour = "n=30; N(2,1/n)" ) ) + -->

<!--   geom_function(fun = dnorm, args = list(mean = 2, sd =sqrt(2/100)), mapping = aes(colour = "n=100; N(2,1/n)" ) )  -->

<!-- ``` -->

<!-- ::: -->

<!-- ## TCL (Prova) {.scrollable} -->

<!-- O TCL afirma que $Z_n = \sqrt{n}(\bar{X}_n - \mu)/\sigma \xrightarrow{D} N(0,1)$. -->

<!-- . . . -->

<!-- Basta mostrar que -->

<!-- $$M_{Z_n}(t) \xrightarrow[n\to\infty]{} e^{t^2/2}$$ -->

<!-- . . . -->

<!-- Seja $Y_i = (X_i - \mu) / \sigma$, então $Z_n = n^{-1/2} \sum_i {Y_i}$ -->

<!-- ## TCL (Prova) {.scrollable} -->

<!-- Seja $M_{Y_i}(t) = M_Y(t), \, \forall i$ a f.g.m. de $Y_i, \, i=1,2,\ldots,n$. -->

<!-- $$M_{Z_n}(t) = \prod_i M_{Y_i}(t/\sqrt{n}) = \left( M_{Y}(t/\sqrt{n}) \right)^n$$ -->

<!-- . . . -->

<!-- Como $Y_i = (X_i-\mu)/\sigma$,então $\mathbb{E}[Y] = 0$ e $\mathbb{E}[Y^2] = \mathbb{V}[Y] = 1$, logo -->

<!-- $$M_{Y}^{(0)}(0) = 1 \qquad  M_{Y}^{(1)}(0) = 0 \qquad  M_{Y}^{(2)}(0) = 1$$ -->

<!-- . . . -->

<!-- Expandindo $M_Y(t)$ em torno de $0$ -->

<!-- $$M_{Y}(t) = \sum_{k=0}^{\infty} \frac{t^k}{k!} M_{Y}^{(k)}(0) \\ = 1 + 0 + t^2 / 2 + \sum_{k=3}^{\infty} \frac{t^k}{k!} M_{Y}^{(k)}(0)$$ -->

<!-- ## TCL (Prova) {.scrollable} -->

<!-- Usando a expansão e calculando $M_Y(t / \sqrt{n})$ -->

<!-- $$M_{Y}(t/ \sqrt{n}) = 1 + (t/\sqrt{n})^2 / 2 + \sum_{k=3}^{\infty} \frac{(t/\sqrt{n})^k}{k!} M_{Y}^{(k)}(0)$$ -->

<!-- . . . -->

<!-- $$ = 1 + \frac{1}{n} \left( t^2 / 2 + \sum_{k=3}^{\infty} \frac{t^k}{ n^{(k-1)/2} k!} M_{Y}^{(k)}(0) \right)$$ -->

<!-- . . . -->

<!-- Lembrando que -->

<!-- $$M_{Z_n}(t) = \left(M_{Y}(t/\sqrt{n})\right)^n = \left( 1 + \frac{t^2/2 + A_n}{n} \right)^n$$ Notem que $A_n \to 0$ quando $n\to \infty$ -->

<!-- ## TCL (Prova) {.scrollable} -->

<!-- Tomando o limite -->

<!-- $$\lim_{n\to \infty}M_{Z_n}(t) = \lim_{n\to \infty}\left( 1 + \frac{t^2/2 + A_n}{n} \right)^n$$ -->

<!-- . . . -->

<!-- { Resultado: se $a_n \to a$, então $\lim_{n\to\infty} (1 + \frac{a_n}{n})^n = e^a$ } -->

<!-- $$ = e^{\frac{t^2}{2}}$$ -->

<!-- Que é a f.g.m. da distribuição $N(0,1)$. C.Q.D.. -->

## Variância desconhecida {.scrollable}

Infelizmente, na prática, $\sigma^2$ não é conhecido.

. . .

Podemos **estimar** $\sigma^2$, e um **estimador** para $\sigma^2$ é dado por $$ S_n^2 = \frac{\sum_{i=1}^n (X_i - \bar{X})^2}{n-1}$$

. . .

Notem que um possível estimador para $\mu$ é $\bar{X}_n$.

. . .

Substituindo, em $Z_n$, $\sigma^2$ por $S_n^2$, temos que

$$T_n = \sqrt{n} \frac{(\bar{X} - \mu)}{S_n^2}$$

## Caso normal

::::: columns
::: {.column width="70%"}
Seja $X_1, X_2, \ldots, X_n$ variáveis aleatórias independentes **normalmente** distribuídas com média, $\mathbb{E}[X_i] = \mu$ e variância, $\sigma^2$. Então $$T_n = \frac{(\bar{X}_n - \mu)}{S_n/\sqrt{n}} \sim t_{n-1}$$
:::

::: {.column width="30%"}
![](extra/gosset.jpg){width="60%"} [![](extra/paper2.png){width="100%"}](https://doi.org/10.2307/2331554)
:::
:::::

## Distribuição $t$

Se $T \sim t_{\nu}$ então $$f_{T}(t) = \frac{\Gamma((\nu+1)/2)}{\sqrt{\pi \nu}\Gamma(\nu/2)}\left(1 +\frac{t^2}{\nu}\right)^{-(\nu+1)/2}, \quad \nu>0.$$

::: incremental
-   $\mathbb{E}[T] = 0$ se $\nu>1$.
-   A distribuição $t_1$ é chamada de distribuição de Cauchy.
-   $\mathbb{V}[T] = \frac{\nu}{\nu-2}$ se $\nu>2$.
-   $T \sim t_\nu \xrightarrow[\nu \to \infty]{} N(0,1)$
:::

## A distribuição $t$ de Student

```{r}
library(ggplot2)
ggplot() +
  geom_function(fun = dnorm, mapping = aes(colour = "N(0,1)" ) ) +
  geom_function(fun = dt, args = list(df = 1), mapping = aes(colour = "t(1)" ) ) +
  geom_function(fun = dt, args = list(df = 2), mapping = aes(colour = "t(2)" ) ) +
  geom_function(fun = dt, args = list(df = 4), mapping = aes(colour = "t(4)" ) ) +
  geom_function(fun = dt, args = list(df = 50), mapping = aes(colour = "t(50)" ) ) +
  geom_function(fun = dt, args = list(df = 90), mapping = aes(colour = "t(90)" ) ) +
  # scale_color_manual(values = 1:5, breaks =  1:5, labels = labels) +
  xlim(-3,3) +
  labs(
    x = "x",
    y = "f(x)",
    colour = "",
  ) + 
  theme_bw(base_size = 16) + 
    theme(legend.position = c(.8,.75))
```

## Caso normal

::::: columns
::: {.column width="60%"}
$$T_n = \frac{(\bar{X}_n - \mu)}{S_n/\sqrt{n}} = \frac{\sigma}{\sigma}\frac{(\bar{X}_n - \mu)}{S_n/\sqrt{n}} \\ = \frac{Z_n}{\sqrt{V_n / n}}$$ onde $V_n = \frac{(n-1)S^2_n}{\sigma^2}$.
:::

::: {.column width="40%"}
![](extra/fisher.png){width="90%"} [![](extra/paper3.png){width="100%"}](https://web.archive.org/web/20160305130235/http://www.sothis.ro/user/content/4ef6e90670749a86-student_distribution_1925.pdf)
:::
:::::

. . .

$$Z_n \sim N(0,1), \qquad \frac{(n-1)V_n}{\sigma^2} \sim \chi^2_{n-1}$$

## TCL (Variância desconhecida) {.scrollable}

Seja $X_1, X_2, \ldots, X_n$ variáveis aleatórias **independentes e identicamente distribuídas** com média, $\mathbb{E}[X_i] = \mu$ e variância, $\sigma^2 < \infty$. Seja $\bar{X}_n = \frac{\sum_{i=1}^{n} X_i}{n}$ e $S_n^2 = \sum_{i=1}^n(X_i - \bar{X})/(n-1)$. Então

$$T_n = \frac{\sqrt{n}(\bar{X}_n - \mu)}{S_n}  \xrightarrow[n\to \infty]{D} N(0,1).$$

## Teorema central do limite (Amostras sem reposição)

::::: columns
::: {.column width="70%"}
Seja $X_1,X_2,\ldots,X_n$ v.a.s selecionadas **sem reposição de uma população finita**. $$\frac{(\bar{X}_n - \mu)}{S_n/\sqrt{n}} \xrightarrow[n\to \infty]{D} N(0,1)$$

[![](extra/paper.png)](https://old.renyi.hu/~p_erdos/1959-13.pdf)
:::

::: {.column width="30%"}
![](extra/erdos.jpg){width="60%"} ![](extra/renyi.jpg){width="50%"}
:::
:::::

## Método Delta

Suponha que $$\frac{\sqrt{n}(Y_n - \mu)}{\sigma} \xrightarrow{D} N(0,1)$$ e que $g$ seja uma função tal que $g'(\mu) \neq 0$. Então

$$\frac{\sqrt{n}(g(Y_n) - g(\mu))}{|g'(\mu)|\sigma} \xrightarrow{D} N(0,1)$$

## Método Delta

Ou seja, $Y_n \xrightarrow{D} N(\mu, \sigma^2/n)$ então $$g(Y_n) \xrightarrow{D} N(g(\mu), g'(\mu)^2\sigma^2/n)$$

## Método Delta (exemplo) {.scrollable}

Exemplo. Suponha que $X_1,X_2,\ldots,X_n$ sejam v.a. independentes de uma distribuição Bernoulli($\theta$). Encontre as distribuições assintóticas de $\bar{X}_n$ e de $Y_n = \frac{\bar{X}_n}{1-\bar{X}_n}$?

. . .

Pelo TCL

$$\bar{X}_n \xrightarrow N(\theta, \theta(1-\theta) / n)$$

. . .

Note que $Y_n = g(\bar{X}_n)$, onde $g(t) = \frac{t}{1-t}$.

. . .

Logo $g'(t) = \frac{1}{(1-t)^2}$. Aplicando o método Delta tem-se que

. . .

$$Y_n \xrightarrow{D} N\left(\frac{\theta}{1-\theta}, \frac{\theta (1-\theta)}{n(1-\theta)^4} \right)$$

## Método Delta (visualizando)

::: panel-tabset
### $\bar{X_n}$

```{r}
n = c(10, 30, 50)
px <- ggplot() +
  geom_function(fun = dnorm, args = list(mean = 0.75, sd = sqrt(0.25 * (1-0.25) / n[1]) ), mapping = aes(colour = "n=10" ) ) +
  geom_function(fun = dnorm, args = list(mean = 0.75, sd = sqrt(0.25 * (1-0.25) / n[2]) ), mapping = aes(colour = "n=30" ) ) +
  geom_function(fun = dnorm, args = list(mean = 0.75, sd = sqrt(0.25 * (1-0.25) / n[3]) ), mapping = aes(colour = "n=50" ) ) +
  geom_vline(xintercept = 0.75, linetype=2) + 
  # geom_function(fun = dgamma, args = list(shape = param[2,1], scale = param[2,2]), 
  #               mapping = aes(colour = "2", linetype ="2" ) ) +
  # geom_function(fun = dgamma, args = list(shape = param[3,1], scale = param[3,2]), 
  #               mapping = aes(colour = "3", linetype ="3" ) ) +
  # geom_function(fun = dgamma, args = list(shape = param[4,1], scale = param[4,2]), 
  #               mapping = aes(colour = "4", linetype ="4" ) ) +
  # geom_function(fun = dgamma, args = list(shape = param[5,1], scale = param[5,2]), 
  #               mapping = aes(colour = "5", linetype ="5" ) ) +
  # scale_color_manual(values = 1:5, breaks =  1:5, labels = labels) +
  # scale_linetype_manual(values = 1:5, breaks =  1:5, labels = labels) +
  xlim(1/2, 1) +
  labs(
    x = "x",
    y = "f(x)",
    colour = "",
  ) + 
  theme_bw(base_size = 16) + 
    theme(legend.position = c(.8,.75))

px
```

### $Y_n = \bar{X_n}/(1-\bar{X_n})$

```{r}
py <- ggplot() +
  geom_function(fun = dnorm, args = list(mean = 0.75 / (1-0.75), sd = sqrt(0.75 * (1-0.75)  / (n[1] * (1-0.75)^4) ) ), mapping = aes(colour = "n=10" ) ) +
  geom_function(fun = dnorm, args = list(mean = 0.75 / (1-0.75), sd = sqrt(0.75 * (1-0.75)  / (n[2] * (1-0.75)^4) ) ), mapping = aes(colour = "n=30" ) ) +
  geom_function(fun = dnorm, args = list(mean = 0.75 / (1-0.75), sd = sqrt(0.75 * (1-0.75)  / (n[3] * (1-0.75)^4) ) ), mapping = aes(colour = "n=50" ) ) +
  geom_vline(xintercept = 0.75/0.25, linetype=2) + 
  # geom_function(fun = dgamma, args = list(shape = param[2,1], scale = param[2,2]), 
  #               mapping = aes(colour = "2", linetype ="2" ) ) +
  # geom_function(fun = dgamma, args = list(shape = param[3,1], scale = param[3,2]), 
  #               mapping = aes(colour = "3", linetype ="3" ) ) +
  # geom_function(fun = dgamma, args = list(shape = param[4,1], scale = param[4,2]), 
  #               mapping = aes(colour = "4", linetype ="4" ) ) +
  # geom_function(fun = dgamma, args = list(shape = param[5,1], scale = param[5,2]), 
  #               mapping = aes(colour = "5", linetype ="5" ) ) +
  # scale_color_manual(values = 1:5, breaks =  1:5, labels = labels) +
  # scale_linetype_manual(values = 1:5, breaks =  1:5, labels = labels) +
  xlim(0,7.5) +
  labs(
    x = "y",
    y = "f(y)",
    colour = "",
  ) + 
  theme_bw(base_size = 16) + 
    theme(legend.position = c(.8,.75))
 
py
```
:::

## Método Delta (simulando)

```{r echo=T}
M = 1000
x <- matrix(rbinom(50*M,size = 1,prob = .75),ncol = M)
```

::: panel-tabset
### $\bar{X_n}$

```{r}
library(tibble)
tibble(x = apply(x,2,FUN = function(x) mean(x))) |> 
  ggplot() + 
  geom_histogram(aes(x=x, y=..density..), bins = 20, fill="white", color=1) + 
  geom_function(fun = dnorm, args = list(mean = 0.75, sd = sqrt(0.25 * (1-0.25) / n[3]) ) ) +
  geom_vline(xintercept = 0.75, linetype=2) + 
  theme_bw()
```

### $Y_n = \bar{X_n}/(1-\bar{X_n})$

```{r}
ggplot() + 
  geom_histogram(data = tibble(y = apply(x,2,FUN = function(x) mean(x)/(1-mean(x)))), mapping = aes(x=y, y=..density..), bins = 20, fill="white", color=1) + 
  geom_function(fun = dnorm, args = list(mean = 0.75 / (1-0.75), sd = sqrt(0.75 * (1-0.75)  / (n[3] * (1-0.75)^4) ) ) ) +  
  geom_vline(xintercept = 0.75/0.25, linetype=2) + 
  theme_bw()

```
:::

# Inferência estatística

## Introdução à inferência

**Inferência estatística** ou **aprendizagem estatística** é o processo de usar os dados para **inferir** (**aprender** a respeito de) a distribuição geradora dos dados.

. . .

Uma típica questão de inferência estatística é:

Dado uma amostra $X_1,X_2,\ldots,X_n \sim F$, como nós inferimos a respeito de $F$?

. . .

Em muitos casos, queremos saber apenas alguma característica de $F$ como a média por exemplo.

## Modelos tipos de modelo

Um **modelo estatístico** $\mathfrak{F}$ é um conjunto de distribuições.

. . .

Um modelo estatístico pode ser classificado como

::: incremental
-   **Paramétrico** (Quando é caracterizado por um conjunto finito de parâmetros)
-   **Não paramétrico** (Quando não é possível caracterizar por um conjunto finito de parâmetros)
:::

## Modelo paramétrico

Um **modelo paramétrico** é um conjunto $\mathfrak{F}$ parametrizado por um número finito de parâmetros. Ex. $$\mathfrak{F} = \left\{ f(x; \mu, \sigma^2) = \frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{1}{2\sigma}(x-\mu)^2}, \quad \mu \in \mathbb{R},\, \sigma^2> 2\right\}$$

. . .

De forma geral, um modelo paramétrico é dado por

$$\mathfrak{F} = \left\{ f(x; \theta) \, : \, \theta \in \Theta \right\}$$

## Modelo não paramétrico

Dizemos que um modelo estatístico é **não paramétrico** se não é possível caracterizá-lo com um número finito de parâmetros.

. . .

Por exemplo, seja $X_1,X_2,\ldots,X_n$ variáveis aleatórias independentes com função acumulada $F$. Estimar $F$ assumindo apenas que $F \in \mathfrak{F}_{ALL} = \{\mbox{todas possíveis f.d.a.s}\}$.

## Regressão, previsão e classificação

Suponha que observamos pares $(X_1, Y_1), (X_2, Y_2), \ldots, (X_n, Y_n)$

::: incremental

- Usualmente $X$ é chamada de preditor, regressor, variável independente, *feature*
- Enquanto $Y$ é chamada de desfecho, variável resposta, variável dependente.
- Chamamos de função de regressão 
$$r(x) = \mathbb{E}[Y | X = x]$$

::: 


## Regressão, previsão e classificação

::: incremental
- Se assumirmos que $r \in \mathfrak{F}$ onde $\mathfrak{F}$ tem dimensão finita, então temos um **modelo paramétrico**.
  - Ex. $r(x) = a + bx$
  
- Se $r \in \mathfrak{F}$ onde $\mathfrak{F}$ não tem dimensão finita, então temos um **modelo não paramétrico**
  - Ex. $r(x) = a + f(x)$ onde $f \in \mathfrak{F}_{SOB}$. 
  $$ \mathfrak{F}_{SOB} =\left\{ f : \int (f''(x))^2dx < \infty \right\}$$
:::
