---
title: "MD21 Introdução à Estatística"
subtitle: "Valor esperado, desigualdades e teoria assintótica"
author: "Leo Bastos"
format: 
  revealjs:
    slide-number: true
    logo: ../../impa-tech-h.png
    footer: '<https://github.com/lsbastos/md21/>'
    theme: simple
    chalkboard: true
editor: visual
---

## Matriz de variancias e covariâncias

Sejam $X = (X_1,X_2,\ldots,X_n)$ um vetor com $n$ variáveis aleatórias. Defina $\mathbb{V}(X_i) = \sigma_i^2$, $\forall i$ e $\mathbb{Cov}(X_i, X_j) = \sigma_{ij}$, $\forall i\neq j$.

. . .

A matriz de variâncias e covariâncias é dada por

$$\Sigma = \left(
\begin{array}{cc}
\sigma_{1}^2 & \sigma_{12} & \ldots & \sigma_{1n} \\
\sigma_{1,2} & \sigma_{2}^2 & \ldots & \sigma_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
\sigma_{n,1} & \sigma_{n,2} & \ldots & \sigma_{n}^2 
\end{array}
\right)$$

## Propriedades

Se $a = (a_1,\ldots,a_n)$ é um vetor de constantes e $X = (X_1,X_2,\ldots,X_n)$ um vetor aleatório com média $\mu = (\mu_1,\mu_2,\ldots,\mu_n)$ e matriz de variância $\Sigma$. Então

$$\mathbb{E}[a^TX] = a^T\mu \qquad \qquad  \mathbb{V}[a^TX] = a^T\Sigma a$$

. . .

Se $A$ for uma matriz $k\times n$, então
$$\mathbb{E}[AX] = A\mu \qquad \qquad  \mathbb{V}[AX] = A\Sigma A^T.$$



## Normal-multivariada

Dizemos que o vetor $X$ seguem uma distribuição normal multivariada, $X \sim N(\mu, \Sigma)$, se $$f_X(x) = \frac{1}{(2\pi)^k |\Sigma|^{1/2} } \exp\left\{ -\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)\right\}$$ onde $|\Sigma|$ denota o determinante de $\Sigma$, $\mu$ é um vetor de tamanho $n$ e $\Sigma$ uma matriz $n\times n$ simétrica, positiva definida.

## Propriedades da normal multivariada

Suponha que o vetor $X$ é particionado em dois vetor $X_1$ e $X_2$, $X = (X_1, X_2)$

. . .

De forma similar $\mu=(\mu_1, \mu_2)$ e $$\Sigma = \left( 
\begin{array}{cc} 
\Sigma_{11} & \Sigma_{12} \\
\Sigma_{21} & \Sigma_{22} 
\end{array}\right)$$

## Propriedades da normal multivariada

Se $X \sim N(\mu, \Sigma)$. Então

::: incremental
1.  A distribuição marginal: $X_1 \sim N(\mu_1, \Sigma_{11})$
2.  A distribuição condicional: $X_2 | X_1 = x_1 \sim N(\mu_{2|1}, \Sigma_{2|1})$ onde $$\mu_{2|1} = \mu_2 + \Sigma_{21} \Sigma_{11}^{-1}(x_1-\mu_1) \\ 
    \Sigma_{2|1} = \Sigma_{22} - \Sigma_{21}\Sigma_{11}^{-1}\Sigma_{12}$$
3. Se $a$ é um vetor, $a^TX \sim Normal(a^T\mu, a^T \Sigma a)$
4. $V = (x - \mu)^T\Sigma^{-1}(x-\mu) \sim \chi^2_n$
:::

## Esperança condicional

<!-- Quarto enables you to weave together content and executable code into a finished presentation. To learn more about Quarto presentations see <https://quarto.org/docs/presentations/>. -->

<!-- <https://quarto.org/docs/presentations/revealjs/> -->

<!-- <https://github.com/quarto-dev/quarto-web/blob/main/docs/presentations/revealjs/demo/index.qmd> -->

A esperança condicional de $X$ dado $Y=y$ é

$$\mathbb{E}[X | Y=y] = \int x f_{X|Y=y}(x|Y=y) dx$$

. . .

Se $h(x,y)$ é uma função de $X$ e $Y$ então

$$\mathbb{E}[h(X,Y) | Y=y] = \int h(x,y) f_{X|Y=y}(x|Y=y) dx$$

## Esperança condicional {.scrollable}

Notem que $\mathbb{E}[X]$ é um número (ou uma função dos parâmetros da distribuição), já $\mathbb{E}[X | Y=y]$ é função de $Y$, e portanto também é uma variável aleatória.

. . .

Exemplo. Suponha que sorteamos um valor $y$ de $Y \sim U(0,1)$. Em seguida sorteamos $X | Y=y \sim U( y, 1)$. Qual o valor esperado de $X|Y=x$?

. . .

$$\mathbb{E}[X|Y=y] = \int x f_{X|Y=y}(x|y) dy = \int_{y}^1 x\frac{1}{1-y} dx$$ 

. . .

$$= \frac{1}{(1-y)} \left. \frac{x^2}{2}\right|_y^1 = \frac{1-y^2}{2(1-y)} = \frac{1+y}{2}.$$

## Esperança condicional {.scrollable}

Para as variáveis aleatórias $X$ e $Y$, assumindo que os valores esperados existem, temos que

$$\mathbb{E}[\mathbb{E}[X|Y]] = \mathbb{E}[X] \qquad \qquad \mathbb{E}[\mathbb{E}[Y|X]] = \mathbb{E}[Y]$$

. . .

Prova. $$ \mathbb{E}[\mathbb{E}[X|Y]] =  \int \mathbb{E}[X|Y=y] f_Y(y) dy \\ = \int \int x f_{X|Y=y}(x|y) dx f_Y(y) dy  $$

. . .

$$= \int \int x f_{X,Y}(x,y) dydx = \int x f_{X}(x) dx \\ = \mathbb{E}[X]$$

## Esperança condicional {.scrollable}

No exemplo anterior, onde $Y\sim U(0,1)$ e $X|Y=y \sim U(y,1)$. Quem é $\mathbb{E}[X]$?

. . . 

$$\mathbb{E}[X] = \mathbb{E}[\mathbb{E}[X|Y=y]] = \mathbb{E}[(1+Y)/2] \\ = (1 + (1/2)) / 2 = 3/4.$$
. . .

Exercício. Encontre $\mathbb{E}[X]$ a partir da marginal de $X$. 

## Esperança condicional {.scrollable}

De forma mais geral, para as variáveis aleatórias $X$ e $Y$, assumindo que os valores esperados existem, temos que

$$\mathbb{E}[\mathbb{E}[h(X,Y)|Y=y]] = \mathbb{E}[h(X,y)]$$


## Variancia condicional {.scrollable}

A variância condicional é definida por $$\mathbb{V}(X|Y=y) = \int (X - \mathbb{E}[X|Y])^2 f_{X|Y}(x|y) dx$$

. . .

Teorema. Para duas v.a.s $X$ e $Y$

$$\mathbb{V}(X) = \mathbb{E}[\mathbb{V}(X|Y=y)]  + \mathbb{V}[\mathbb{E}(X|Y=y)]$$

## Função geradora de momentos

A função geradora de momentos, f.g.m, de uma v.a. $X$ é definida por

$$M_X(t) = \psi_X(t) = \mathbb{E}[e^{tX}] = \int e^{tX} f_X(x) dx, \quad t\in \mathbb{R}.$$

. . .

Propriedade 1: Se $M_X(t)$ existe para $t$ em um intervalo aberto contendo zero, então ela determina a distribuição de probabilidade de forma única.

. . .

Propriedade 2: Se $M_X(t)$ existe para $t$ em um intervalo aberto contendo zero, então $M^{(r)}_X(t) = \mathbb{E}[X^r]$.


## Exemplos

F.g.m da exponencial

F.g.m da Poisson ?


F.g.m da Normal

## Propriedades interessantes

Se $Y = aX + b$, então $M_Y(t) = e^{bt}M_X(at)$

. . .

Se $X_1, X_2,\ldots,X_n$ independentes com f.g.m $M_{X_i}(t)$ e $Y=\sum_iX_i$, então $$M_Y(t) = \prod_i M_{X_i}(t)$$

## Exemplo

Soma de Poisson

# Principais desigualdades

## Markov

## Chebyshev

## Jensen

# Teoria assintótica

## Convergencia em probabilidade

## Convergencia em distribuição

## Lei fraca dos grandes números

## Teorema central do limite

## Método Delta
